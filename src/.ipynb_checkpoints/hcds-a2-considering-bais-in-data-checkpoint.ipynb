{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f8d977a",
   "metadata": {},
   "source": [
    "# Considering Bias in Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64510d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are standard python modules\n",
    "import json, time, urllib.parse\n",
    "#\n",
    "# The 'requests' module is not a standard Python module. You will need to install this with pip/pip3 if you do not already have it\n",
    "import requests\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9adbfa",
   "metadata": {},
   "source": [
    "## Step 1: Getting the Article and Population Data\n",
    "\n",
    "The first step is getting the data, which lives in several different places. You will need data that lists Wikipedia articles of politicians and data for country populations.<br />\n",
    "The Wikipedia Category:Politicians by nationality was crawled to generate a list of Wikipedia article pages about politicians from a wide range of countries.<br />\n",
    "The population data is drawn from the world population data sheet published by the Population Reference Bureau. This csv file had only two columns, Geography and Population. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c058fc63",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>url</th>\n",
       "      <th>country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Shahjahan Noori</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Shahjahan_Noori</td>\n",
       "      <td>Afghanistan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Abdul Ghafar Lakanwal</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Abdul_Ghafar_Lak...</td>\n",
       "      <td>Afghanistan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    name                                                url  \\\n",
       "0        Shahjahan Noori      https://en.wikipedia.org/wiki/Shahjahan_Noori   \n",
       "1  Abdul Ghafar Lakanwal  https://en.wikipedia.org/wiki/Abdul_Ghafar_Lak...   \n",
       "\n",
       "       country  \n",
       "0  Afghanistan  \n",
       "1  Afghanistan  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Politicians by nationality data from the raw_files directory\n",
    "politicians_by_country = pd.read_csv('../raw_files/politicians_by_country_SEPT.2022.csv')\n",
    "politicians_by_country.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7896ed",
   "metadata": {},
   "source": [
    "The population_by_country_2022.csv contains some rows that provide cumulative regional population counts. These rows are distinguished by having ALL CAPS values in the 'geography' field (e.g. AFRICA, OCEANIA). These rows won't match the country values in politicians_by_country.SEPT.2022.csv. For the sake of report coverage and analysis, I re-organized the data into four columns namely, continent, region, geography and population.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8f2ea189",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Continent</th>\n",
       "      <th>Region</th>\n",
       "      <th>Geography</th>\n",
       "      <th>Population (millions)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AFRICA</td>\n",
       "      <td>NORTHERN AFRICA</td>\n",
       "      <td>Algeria</td>\n",
       "      <td>44.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AFRICA</td>\n",
       "      <td>NORTHERN AFRICA</td>\n",
       "      <td>Egypt</td>\n",
       "      <td>103.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AFRICA</td>\n",
       "      <td>NORTHERN AFRICA</td>\n",
       "      <td>Libya</td>\n",
       "      <td>6.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AFRICA</td>\n",
       "      <td>NORTHERN AFRICA</td>\n",
       "      <td>Morocco</td>\n",
       "      <td>36.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AFRICA</td>\n",
       "      <td>NORTHERN AFRICA</td>\n",
       "      <td>Sudan</td>\n",
       "      <td>46.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Continent           Region Geography  Population (millions)\n",
       "0    AFRICA  NORTHERN AFRICA   Algeria                   44.9\n",
       "1    AFRICA  NORTHERN AFRICA     Egypt                  103.5\n",
       "2    AFRICA  NORTHERN AFRICA     Libya                    6.8\n",
       "3    AFRICA  NORTHERN AFRICA   Morocco                   36.7\n",
       "4    AFRICA  NORTHERN AFRICA     Sudan                   46.9"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load reorganized country population data from the cleaned directory\n",
    "population_by_country = pd.read_csv('../cleaned/population_by_country_2022_cleaned.csv')\n",
    "population_by_country.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8efd975c",
   "metadata": {},
   "source": [
    "## Step 2: Getting Article Quality Predictions\n",
    "\n",
    "In this step, I need to get the predicted quality scores for each article in the Wikipedia dataset using a machine learning system called ORES. This was originally an acronym for \"Objective Revision Evaluation Service\" but was simply renamed “ORES”. ORES is a machine learning tool that can provide estimates of Wikipedia article quality. The article quality estimates are, from best to worst:\n",
    "\n",
    "- FA - Featured article\n",
    "- GA - Good article\n",
    "- B - B-class article\n",
    "- C - C-class article\n",
    "- Start - Start-class article\n",
    "- Stub - Stub-class article \n",
    "\n",
    "<br/>\n",
    "These were learned based on articles in Wikipedia that were peer-reviewed using the Wikipedia content assessment procedures. These quality classes are a sub-set of quality assessment categories developed by Wikipedia editors.<br/>\n",
    "ORES requires a specific revision ID of a specific article to be able to make a label prediction. You can use the API:Info request to get a range of metadata on an article, including the most current revision ID of the article page.\n",
    "<br/><br/>\n",
    "The code below will try to get a Wikipedia page quality prediction from ORES for each politician’s article page by: \n",
    "\n",
    "- reading each line of politicians_by_country.SEPT.2022.csv, \n",
    "- making a page info request to get the current page revision, and \n",
    "- making an ORES request using the page title and current revision id.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a37bd8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code will access page info data using the \n",
    "# [MediaWiki REST API for the EN Wikipedia](https://www.mediawiki.org/wiki/API:Main_page).\n",
    "#########\n",
    "#\n",
    "#    CONSTANTS\n",
    "#\n",
    "\n",
    "# The basic English Wikipedia API endpoint\n",
    "API_ENWIKIPEDIA_ENDPOINT = \"https://en.wikipedia.org/w/api.php\"\n",
    "\n",
    "# We'll assume that there needs to be some throttling for these requests - we should always be nice to a free data resource\n",
    "API_LATENCY_ASSUMED = 0.002       # Assuming roughly 2ms latency on the API and network\n",
    "API_THROTTLE_WAIT = (1.0/100.0)-API_LATENCY_ASSUMED\n",
    "\n",
    "# When making automated requests we should include something that is unique to the person making the request\n",
    "# This should include an email - your UW email would be good to put in there\n",
    "REQUEST_HEADERS = {\n",
    "    'User-Agent': '<harrymn@uw.edu>, University of Washington, MSDS DATA 512 - AUTUMN 2022',\n",
    "}\n",
    "\n",
    "# This is just a list of English Wikipedia article titles that we can use for example requests\n",
    "ARTICLE_TITLES = [ 'Bison', 'Northern flicker', 'Red squirrel', 'Chinook salmon', 'Horseshoe bat' ]\n",
    "\n",
    "# This is a string of additional page properties that can be returned see the Info documentation for\n",
    "# what can be included. If you don't want any this can simply be the empty string\n",
    "PAGEINFO_EXTENDED_PROPERTIES = \"talkid|url|watched|watchers\"\n",
    "#PAGEINFO_EXTENDED_PROPERTIES = \"\"\n",
    "\n",
    "# This template lists the basic parameters for making this\n",
    "PAGEINFO_PARAMS_TEMPLATE = {\n",
    "    \"action\": \"query\",\n",
    "    \"format\": \"json\",\n",
    "    \"titles\": \"\",           # to simplify this should be a single page title at a time\n",
    "    \"prop\": \"info\",\n",
    "    \"inprop\": PAGEINFO_EXTENDED_PROPERTIES\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efdb4492",
   "metadata": {},
   "source": [
    "The API request will be made using one procedure. The idea is to make this reusable. The procedure is parameterized, but relies on the constants above for the important parameters. The underlying assumption is that this will be used to request data for a set of article pages. Therefore the parameter most likely to change is the article_title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a43a7dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########\n",
    "#\n",
    "#    PROCEDURES/FUNCTIONS\n",
    "#\n",
    "\n",
    "def request_pageinfo_per_article(article_title = None, \n",
    "                                 endpoint_url = API_ENWIKIPEDIA_ENDPOINT, \n",
    "                                 request_template = PAGEINFO_PARAMS_TEMPLATE,\n",
    "                                 headers = REQUEST_HEADERS):\n",
    "    # Make sure we have an article title\n",
    "    if not article_title: return None\n",
    "    \n",
    "    request_template['titles'] = article_title\n",
    "        \n",
    "    # make the request\n",
    "    try:\n",
    "        # we'll wait first, to make sure we don't exceed the limit in the situation where an exception\n",
    "        # occurs during the request processing - throttling is always a good practice with a free\n",
    "        # data source like Wikipedia - or any other community sources\n",
    "        if API_THROTTLE_WAIT > 0.0:\n",
    "            time.sleep(API_THROTTLE_WAIT)\n",
    "        response = requests.get(endpoint_url, headers=headers, params=request_template)\n",
    "        json_response = response.json()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        json_response = None\n",
    "    return json_response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6359b26a",
   "metadata": {},
   "source": [
    "This functions below generate quality scores for article revisions using [ORES](https://www.mediawiki.org/wiki/ORES).The API documentation can be access from the main [ORES](https://ores.wikimedia.org) page. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6ac9ce76",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########\n",
    "#\n",
    "#    CONSTANTS\n",
    "#\n",
    "\n",
    "# The current ORES API endpoint\n",
    "API_ORES_SCORE_ENDPOINT = \"https://ores.wikimedia.org/v3\"\n",
    "# A template for mapping to the URL\n",
    "API_ORES_SCORE_PARAMS = \"/scores/{context}/{revid}/{model}\"\n",
    "\n",
    "# Use some delays so that we do not hammer the API with our requests\n",
    "API_LATENCY_ASSUMED = 0.002       # Assuming roughly 2ms latency on the API and network\n",
    "API_THROTTLE_WAIT = (1.0/100.0)-API_LATENCY_ASSUMED\n",
    "\n",
    "# When making automated requests we should include something that is unique to the person making the request\n",
    "# This should include an email - your UW email would be good to put in there\n",
    "REQUEST_HEADERS = {\n",
    "    'User-Agent': '<harrymn@uw.edu>, University of Washington, MSDS DATA 512 - AUTUMN 2022'\n",
    "}\n",
    "\n",
    "# A dictionary of English Wikipedia article titles (keys) and sample revision IDs that can be used for this ORES scoring example\n",
    "ARTICLE_REVISIONS = { 'Bison':1085687913 , 'Northern flicker':1086582504 , 'Red squirrel':1083787665 , 'Chinook salmon':1085406228 , 'Horseshoe bat':1060601936 }\n",
    "\n",
    "# This template lists the basic parameters for making an ORES request\n",
    "ORES_PARAMS_TEMPLATE = {\n",
    "    \"context\": \"enwiki\",        # which WMF project for the specified revid\n",
    "    \"revid\" : \"\",               # the revision to be scored - this will probably change each call\n",
    "    \"model\": \"articlequality\"   # the AI/ML scoring model to apply to the reviewion\n",
    "}\n",
    "#\n",
    "# The current ML models for English wikipedia are:\n",
    "#   \"articlequality\"\n",
    "#   \"articletopic\"\n",
    "#   \"damaging\"\n",
    "#   \"version\"\n",
    "#   \"draftquality\"\n",
    "#   \"drafttopic\"\n",
    "#   \"goodfaith\"\n",
    "#   \"wp10\"\n",
    "#\n",
    "# The specific documentation on these is scattered so if you want to use one you'll have to look around.\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f091f7",
   "metadata": {},
   "source": [
    "The API request will be made using one procedure. The idea is to make this reusable. The procedure is parameterized, but relies on the constants above for the important parameters. The underlying assumption is that this will be used to request data for a set of article revisions. Therefore, the main parameter is article_revid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "93fac518",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########\n",
    "#\n",
    "#    PROCEDURES/FUNCTIONS\n",
    "#\n",
    "\n",
    "def request_ores_score_per_article(article_revid = None, \n",
    "                                   endpoint_url = API_ORES_SCORE_ENDPOINT, \n",
    "                                   endpoint_params = API_ORES_SCORE_PARAMS, \n",
    "                                   request_template = ORES_PARAMS_TEMPLATE,\n",
    "                                   headers = REQUEST_HEADERS,\n",
    "                                   features=False):\n",
    "    # Make sure we have an article revision id\n",
    "    if not article_revid: return None\n",
    "    \n",
    "    # set the revision id into the template\n",
    "    request_template['revid'] = article_revid\n",
    "    \n",
    "    # now, create a request URL by combining the endpoint_url with the parameters for the request\n",
    "    request_url = endpoint_url+endpoint_params.format(**request_template)\n",
    "    \n",
    "    # the features used by the ML model can sometimes be returned as well as scores\n",
    "    if features:\n",
    "        request_url = request_url+\"?features=true\"\n",
    "    \n",
    "    # make the request\n",
    "    try:\n",
    "        # we'll wait first, to make sure we don't exceed the limit in the situation where an exception\n",
    "        # occurs during the request processing - throttling is always a good practice with a free\n",
    "        # data source like ORES - or other community sources\n",
    "        if API_THROTTLE_WAIT > 0.0:\n",
    "            time.sleep(API_THROTTLE_WAIT)\n",
    "        response = requests.get(request_url, headers=headers)\n",
    "        json_response = response.json()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        json_response = None\n",
    "    return json_response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657d23bf",
   "metadata": {},
   "source": [
    "This process will loop through all articles (politicians by country) and try to retreive their revision counts. If an article is found without a revision, it is saved inside a list called ARTICLE_NO_REVISION. All articles with revisions are saved inside a dictionary with the name ARTICLE_REVISIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a758607d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Bison': 1115011990,\n",
       " 'Northern flicker': 1112029621,\n",
       " 'Red squirrel': 1110629492,\n",
       " 'Chinook salmon': 1114242809,\n",
       " 'Horseshoe bat': 1101796831}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ARTICLE_NO_REVISION = []\n",
    "ARTICLE_REVISIONS = {}\n",
    "for i in range(0, len(ARTICLE_TITLES)):\n",
    "    info = request_pageinfo_per_article(ARTICLE_TITLES[i])\n",
    "    obj = info['query']['pages']\n",
    "    info_key = list(obj.keys())[0]\n",
    "    revision_id = info['query']['pages'][info_key]['lastrevid']\n",
    "    # Check if article as a revision\n",
    "    if revision_id and revision_id>0:\n",
    "        # Update ARTICLE_REVISIONS dict with the article and last revision number\n",
    "        ARTICLE_REVISIONS.update({ARTICLE_TITLES[i]:revision_id})\n",
    "    else:\n",
    "        # update the list of articles with no revision\n",
    "        ARTICLE_NO_REVISION.append(ARTICLE_TITLES[i])\n",
    "ARTICLE_REVISIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d463560",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
